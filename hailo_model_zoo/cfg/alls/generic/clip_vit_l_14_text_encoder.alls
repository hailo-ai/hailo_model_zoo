model_optimization_config(calibration, batch_size=8, calibset_size=64)
model_optimization_config(globals, multiproc_policy=disabled)
pre_quantization_optimization(ew_add_fusing, policy=disabled)
model_optimization_flavor(optimization_level=0, compression_level=0)
pre_quantization_optimization(matmul_correction, layers={matmul*}, correction_type=zp_comp_block)
quantization_param(input_layer1, precision_mode=a16_w16)
quantization_param({ew_add*}, precision_mode=a16_w16)
quantization_param({normalization*}, precision_mode=a16_w16)
quantization_param({conv*}, precision_mode=a16_w16)
quantization_param(conv1, precision_mode=a8_w8)
quantization_param(conv3, precision_mode=a8_w8)
quantization_param(conv7, precision_mode=a8_w8)
quantization_param(conv9, precision_mode=a8_w8)
quantization_param(conv13, precision_mode=a8_w8)
quantization_param(conv15, precision_mode=a8_w8)
quantization_param(conv19, precision_mode=a8_w8)
quantization_param(conv21, precision_mode=a8_w8)
quantization_param(conv25, precision_mode=a8_w8)
quantization_param(conv27, precision_mode=a8_w8)
quantization_param(conv31, precision_mode=a8_w8)
quantization_param(conv33, precision_mode=a8_w8)
quantization_param(conv37, precision_mode=a8_w8)
quantization_param(conv39, precision_mode=a8_w8)
quantization_param(conv43, precision_mode=a8_w8)
quantization_param(conv45, precision_mode=a8_w8)
quantization_param(conv49, precision_mode=a8_w8)
quantization_param(conv51, precision_mode=a8_w8)
quantization_param(conv55, precision_mode=a8_w8)
quantization_param(conv57, precision_mode=a8_w8)
quantization_param(conv61, precision_mode=a8_w8)
quantization_param(conv63, precision_mode=a8_w8)
quantization_param(conv67, precision_mode=a8_w8)
quantization_param(conv69, precision_mode=a8_w8)
post_quantization_optimization(finetune, policy=enabled, batch_size=4, epochs=8)
pre_quantization_optimization(layer_norm_decomposition, equalization=disabled, bit_decomposition_mode=uniform_precision)
