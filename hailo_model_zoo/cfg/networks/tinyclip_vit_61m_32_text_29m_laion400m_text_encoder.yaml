base:
- base/clip_text_encoder.yaml
postprocessing:
  postprocess_config_file: models_files/ZeroShotClassification/clip/tinyclip/tinyclip_vit_61m_32_text_29m_laion400m_text_encoder/pretrained/2025-07-21/TinyCLIP-ViT-61M-32-Text-29M-LAION400M.npz
evaluation:
  data_set: models_files/ZeroShotClassification/clip/tinyclip/tinyclip_vit_61m_32_text_29m_laion400m_text_encoder/coco_10xtd/2025-07-21/coco_xtd10_en_vit_61m_32.tfrecord
quantization:
  calib_set:
  - models_files/ZeroShotClassification/clip/tinyclip/tinyclip_vit_61m_32_text_29m_laion400m_text_encoder/coco_10xtd/2025-07-21/coco_xtd10_en_vit_61m_32.tfrecord
network:
  network_name: tinyclip_vit_61m_32_text_29m_laion400m_text_encoder
paths:
  network_path:
  - models_files/ZeroShotClassification/clip/tinyclip/tinyclip_vit_61m_32_text_29m_laion400m_text_encoder/pretrained/2025-07-21/TinyCLIP-ViT-61M-32-Text-29M-LAION400M.sim.onnx
  url: https://hailo-model-zoo.s3.eu-west-2.amazonaws.com/ZeroShotClassification/clip/tinyclip/tinyclip_vit_61m_32_text_29m_laion400m_text_encoder/pretrained/2025-07-21/TinyCLIP-ViT-61M-32-Text-29M-LAION400M_text_encoder.zip
  alls_script: tinyclip_vit_61m_32_text_29m_laion400m_text_encoder.alls
parser:
  nodes:
  - /embeddings/Add
  - - /encoder/layers.8/Add_1
info:
  input_shape: 1x77x512
  output_shape: 1x77x512
  operations: 4.5G
  parameters: 29M
  framework: pytorch
  full_precision_result: 93.8
  source: https://huggingface.co/wkcn/TinyCLIP-ViT-61M-32-Text-29M-LAION400M
  license_url: https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/mit.md
  license_name: MIT
