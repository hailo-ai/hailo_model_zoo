norm1 = normalization([122.7709383, 116.7460125, 104.09373615000001], [68.5005327, 66.6321579, 70.32316304999999])
model_optimization_flavor(optimization_level=0, compression_level=0)
pre_quantization_optimization(ew_add_fusing, policy=disabled)
quantization_param({format_conversion1}, precision_mode=a16_w16)
pre_quantization_optimization(matmul_correction, layers={matmul*}, correction_type=zp_comp_block)
pre_quantization_optimization(matmul_equalization, layers={matmul*}, policy=enabled, matmul_bias=enabled)
quantization_param({conv1}, precision_mode=a16_w16)
quantization_param({conv50}, precision_mode=a16_w16)
quantization_param({ew_add*}, precision_mode=a16_w16)
quantization_param({norm*}, precision_mode=a16_w16)
quantization_param([slice1], precision_mode=a16_w16)
quantization_param({conv_feature_splitter*_3}, precision_mode=a8_w8) 
model_optimization_config(calibration, calibset_size=64)
post_quantization_optimization(finetune, policy=disabled, learning_rate=0.000001)
model_optimization_config(globals, multiproc_policy=disabled)

performance_param(compiler_optimization_level=max)