normalization1 = normalization([0.0, 0.0, 0.0], [255.0, 255.0, 255.0])
model_optimization_config(calibration, batch_size=4, calibset_size=64)
# post_quantization_optimization commands
post_quantization_optimization(bias_correction, policy=disabled)
pre_quantization_optimization(equalization, policy=enabled)
post_quantization_optimization(finetune, policy=enabled, use_acceleras=disabled, dataset_size=4000, epochs=8, learning_rate=1e-5, loss_factors=[1.0, 2.0, 0.25, 0.125, 1.0, 2.0, 0.25, 0.125, 1.0, 2.0, 0.25, 0.125], loss_layer_names=[conv110_centers, conv110_scales, conv110_obj, conv110_probs, conv103_centers, conv103_scales, conv103_obj, conv103_probs, conv95_centers, conv95_scales, conv95_obj, conv95_probs], loss_types=[l2, l2, l2, l2, l2, l2, l2, l2, l2, l2, l2, l2])

context_switch_param(max_memory_utilization=0.45, max_control_utilization=0.45, max_compute_utilization=0.45)
resources_param(max_control_utilization=0.7, max_memory_utilization=0.7, max_compute_utilization=0.75)
