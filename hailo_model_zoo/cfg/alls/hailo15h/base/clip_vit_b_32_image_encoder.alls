clip_vit_b_32_image_encoder/norm1 = normalization([122.7709383, 116.7460125, 104.09373615000001], [68.5005327, 66.6321579, 70.32316304999999])
model_optimization_flavor(optimization_level=0,compression_level=0)
pre_quantization_optimization(layer_norm_decomposition, bit_decomposition_mode=uniform_precision, equalization=disabled)
model_optimization_config(calibration, calibset_size=64)
quantization_param({ew_add*}, precision_mode=a16_w16)
quantization_param({norm*}, precision_mode=a16_w16)
quantization_param([slice1], precision_mode=a16_w16)
quantization_param([conv1, format_conversion1], precision_mode=a16_w16)

allocator_param(disable_row_per_cut=True, enable_fixer=max_adjcents, automatic_l4_portals=False, input_layer_max_successors=4, enable_post_split_average_buffers=False)
performance_param(optimize_for_batch=63)
resources_param(max_utilization=1.0)
context_switch_param(toposort_mode=pushdown, slotter_chances=200, mode=enabled, share_buckets_partition=True, auto_partition_to_buckets=True, berkitizer_min_size=200, berkitizer_bucket_size=28)
