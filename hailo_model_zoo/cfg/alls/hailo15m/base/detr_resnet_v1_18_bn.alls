norm_layer = normalization([123.675, 116.28, 103.53], [58.395, 57.12, 57.375])
post_quantization_optimization(bias_correction, policy=disabled)
pre_quantization_optimization(equalization, policy=enabled)
model_optimization_flavor(optimization_level=0, compression_level=0)

# model_optimization_config commands
model_optimization_config(negative_exponent, layers={*}, rank=0)
model_optimization_config(calibration,batch_size=8, calibset_size=32)
model_optimization_config(checker_cfg, policy=enabled, batch_size=1)
quantization_param([detr_resnet_v1_18_bn/conv116, detr_resnet_v1_18_bn/conv115], precision_mode=a16_w16)
quantization_param([detr_resnet_v1_18_bn/conv116, detr_resnet_v1_18_bn/conv115], bias_mode=single_scale_decomposition)

quantization_param({ew_sub*}, activation_fit=disabled)

quantization_param({output_layer*}, precision_mode=a16_w16)
change_output_activation(detr_resnet_v1_18_bn/conv116, linear)

allocator_param(enable_partial_row_buffers=disabled)
context_switch_param(slotter_chances=600)
allocator_param(timeout=43200)
compilation_param(conv3, balance_output_multisplit=enabled)