base:
- base/clip.yaml
network:
  network_name: tinyclip_vit_8m_16_text_3m_yfcc15m_image_encoder
postprocessing:
  postprocess_config_file: models_files/cifar100/2025-07-21/class_token_tinyclip_vit_8m_16_text_3m_yfcc15m.npy
paths:
  network_path:
  - models_files/ZeroShotClassification/clip/tinyclip/tinyclip_vit_8m_16_text_3m_yfcc15m_image_encoder/pretrained/2025-07-21/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M_image_encoder.sim.onnx
  url: https://hailo-model-zoo.s3.eu-west-2.amazonaws.com/ZeroShotClassification/clip/tinyclip/tinyclip_vit_8m_16_text_3m_yfcc15m_image_encoder/pretrained/2025-07-21/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M_image_encoder.zip
  alls_script: tinyclip_vit_8m_16_text_3m_yfcc15m_image_encoder.alls
info:
  task: zero-shot classification
  input_shape: 224x224x3
  output_shape: 1x1x512
  operations: 3.6G
  parameters: 8M
  framework: pytorch
  training_data: internal
  validation_data: cifar100
  eval_metric: Accuracy (top1)
  source: https://huggingface.co/wkcn/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M
  full_precision_result: 41.98
  license_url: https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/mit.md
  license_name: MIT
