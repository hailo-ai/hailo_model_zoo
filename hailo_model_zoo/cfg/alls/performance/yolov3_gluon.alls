normalization1 = normalization([123.675, 116.28, 103.53], [58.395, 57.12, 57.375])
model_optimization_config(calibration, batch_size=2, calibset_size=64)
model_optimization_flavor(compression_level=2)
# post_quantization_optimization commands
post_quantization_optimization(finetune, policy=enabled, learning_rate=0.0001, epochs=8, dataset_size=4000)

quantization_param(output_layer1, precision_mode=a16_w16)
quantization_param(output_layer2, precision_mode=a16_w16)
quantization_param(output_layer3, precision_mode=a16_w16)

allocator_param(automatic_ddr=False)
buffer_calc_param(buffer_calc_fps=0.0, optimize_buffers=True, split_aware_optimize_buffers=True)
context_switch_param(goal_network_compute_utilization=0.75, goal_network_control_utilization=1.0, goal_network_memory_utilization=0.75, goal_network_weights_utilization=0.75, max_compute_utilization=0.3, max_control_utilization=0.3, max_memory_utilization=0.5, max_utilization=1.0, mode=allowed)

