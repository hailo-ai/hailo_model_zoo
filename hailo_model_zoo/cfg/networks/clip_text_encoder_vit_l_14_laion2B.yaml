base:
- base/clip_text_encoder.yaml
postprocessing:
  postprocess_config_file: models_files/clip/vit_l_14_laion2B/pretrained/2024-09-24/hg-clip-vit-l-14-laion2b.npz
evaluation:
  data_set:
  - models_files/clip/vit_l_14_laion2B/pretrained/2024-09-24/coco_xtd10_en_clip_vit_l_14_laion2B.tfrecord
quantization:
  calib_set:
  - models_files/clip/vit_l_14_laion2B/pretrained/2024-09-24/coco_xtd10_en_clip_vit_l_14_laion2B.tfrecord
network:
  network_name: clip_text_encoder_vit_l_14_laion2B
parser:
  nodes:
  - Add_4
  - output
paths:
  alls_script: clip_text_encoder_vit_l_14_laion2B.alls
  network_path:
  - models_files/clip/vit_l_14_laion2B/pretrained/2024-09-24/clip_text_encoder_vit_l_14_laion2B.onnx
  url: https://hailo-model-zoo.s3.eu-west-2.amazonaws.com/clip/vit_l_14_laion2B/pretrained/2024-09-24/clip-vit-l-14-laion2b-s32b-b82k_text_op15.zip
info:
  input_shape: 1x77x768
  output_shape: 1x77x768
  operations: 13.85G
  parameters: 78.87M
  framework: pytorch
  full_precision_result: 94.7
  source: https://huggingface.co/laion/CLIP-ViT-L-14-laion2B-s32B-b82K
  license_url: https://github.com/openai/CLIP/blob/main/LICENSE
